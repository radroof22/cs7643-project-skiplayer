{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/cs7643/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer'],\n",
       "    num_rows: 14042\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"all\")\n",
    "mmlu[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add the \"prompt\" field\n",
    "def add_prompt(example):\n",
    "    # Create a formatted string for the choices\n",
    "    formatted_choices = '\\n'.join([f\"({label}) {choice}\" for label, choice in zip(choice_labels, example[\"choices\"])])\n",
    "    # Concatenate the question and the formatted choices into a new field called \"prompt\"\n",
    "    example[\"prompt\"] = example[\"question\"] + \"\\n\" + formatted_choices\n",
    "    return example\n",
    "\n",
    "# Use the .map() method to apply the function to each row in the \"test\" split\n",
    "mmlu[\"test\"] = mmlu[\"test\"].map(add_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n",
       " 'subject': 'abstract_algebra',\n",
       " 'choices': ['0', '4', '2', '6'],\n",
       " 'answer': 1,\n",
       " 'prompt': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\n(0) 0\\n(1) 4\\n(2) 2\\n(3) 6'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 144.70ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14150821"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu[\"test\"].to_json(\"./LayerSkip/custom_datasets/mmlu_test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique subjects from the test split.\n",
    "# You can do this by extracting the \"subject\" column and converting it to a set.\n",
    "unique_subjects = set(mmlu[\"test\"][\"subject\"])\n",
    "\n",
    "# Create the output directory if it doesn't exist.\n",
    "output_folder = \"./LayerSkip/custom_datasets/mmlu/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop over each unique subject and filter the dataset.\n",
    "for subject in unique_subjects:\n",
    "    # Filter the dataset to only the rows for the current subject.\n",
    "    subject_ds = mmlu[\"test\"].filter(lambda example: example[\"subject\"] == subject)\n",
    "    \n",
    "\n",
    "    # Create a safe filename from the subject name.\n",
    "    safe_subject = subject.replace(\" \", \"_\")\n",
    "    output_file = os.path.join(output_folder, f\"{safe_subject}.jsonl\")\n",
    "    \n",
    "    # Save the filtered dataset to a JSONL file.\n",
    "    # Here we export records in a line-delimited JSON format.\n",
    "    subject_ds.to_json(output_file, orient=\"records\", lines=True)\n",
    "    \n",
    "    print(f\"Saved subject '{subject}' to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ-Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load the validation split\n",
    "dataset = load_dataset(\"google-research-datasets/nq_open\", split=\"validation\")\n",
    "\n",
    "# Open a file for writing in line-delimited JSON format\n",
    "with open(\"./LayerSkip/custom_datasets/nq_open_val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dataset:\n",
    "        question = item[\"question\"]\n",
    "        answers = item[\"answer\"]\n",
    "        if answers:\n",
    "            answer = answers[0]\n",
    "            json_line = {\"question\": question, \"answer\": answer}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RACE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import ast\n",
    "\n",
    "dataset = load_dataset(\"EleutherAI/race\", split=\"test\")\n",
    "option_labels = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "with open(\"./LayerSkip/custom_datasets/race_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in dataset:\n",
    "        article = row[\"article\"]\n",
    "        data_str = row[\"problems\"]\n",
    "\n",
    "        try:\n",
    "            data = ast.literal_eval(data_str)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                data = json.loads(data_str.replace(\"'\", '\"'))\n",
    "            except Exception as e:\n",
    "                raise\n",
    "\n",
    "        for item in data:\n",
    "            options_str = \" \".join([f\"{label}. {opt}\" for label, opt in zip(option_labels, item['options'])])\n",
    "            question = f\"Article: {article} Question: {item['question']} Answer Options: {options_str}\"\n",
    "            answer = item[\"answer\"]\n",
    "            if answer:\n",
    "                json_line = {\"question\": question, \"answer\": answer}\n",
    "                f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path  = \"./LayerSkip/custom_datasets/mmlu_test.jsonl\"\n",
    "output_path = \"./LayerSkip/custom_datasets/mmlu_test_fixed.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        # parse the JSON object\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # cast `answer` to string (even if it's already one)\n",
    "        record[\"answer\"] = str(record.get(\"answer\", \"\"))\n",
    "\n",
    "        # write back as a single JSON line\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed college_chemistry.jsonl → college_chemistry_fixed.jsonl\n",
      "Processed college_mathematics.jsonl → college_mathematics_fixed.jsonl\n",
      "Processed professional_accounting.jsonl → professional_accounting_fixed.jsonl\n",
      "Processed philosophy.jsonl → philosophy_fixed.jsonl\n",
      "Processed high_school_statistics.jsonl → high_school_statistics_fixed.jsonl\n",
      "Processed business_ethics.jsonl → business_ethics_fixed.jsonl\n",
      "Processed computer_security.jsonl → computer_security_fixed.jsonl\n",
      "Processed formal_logic.jsonl → formal_logic_fixed.jsonl\n",
      "Processed world_religions.jsonl → world_religions_fixed.jsonl\n",
      "Processed clinical_knowledge.jsonl → clinical_knowledge_fixed.jsonl\n",
      "Processed anatomy.jsonl → anatomy_fixed.jsonl\n",
      "Processed high_school_computer_science.jsonl → high_school_computer_science_fixed.jsonl\n",
      "Processed sociology.jsonl → sociology_fixed.jsonl\n",
      "Processed high_school_us_history.jsonl → high_school_us_history_fixed.jsonl\n",
      "Processed moral_scenarios.jsonl → moral_scenarios_fixed.jsonl\n",
      "Processed marketing.jsonl → marketing_fixed.jsonl\n",
      "Processed high_school_biology.jsonl → high_school_biology_fixed.jsonl\n",
      "Processed high_school_psychology.jsonl → high_school_psychology_fixed.jsonl\n",
      "Processed college_physics.jsonl → college_physics_fixed.jsonl\n",
      "Processed high_school_mathematics.jsonl → high_school_mathematics_fixed.jsonl\n",
      "Processed high_school_geography.jsonl → high_school_geography_fixed.jsonl\n",
      "Processed medical_genetics.jsonl → medical_genetics_fixed.jsonl\n",
      "Processed college_biology.jsonl → college_biology_fixed.jsonl\n",
      "Processed high_school_chemistry.jsonl → high_school_chemistry_fixed.jsonl\n",
      "Processed astronomy.jsonl → astronomy_fixed.jsonl\n",
      "Processed high_school_physics.jsonl → high_school_physics_fixed.jsonl\n",
      "Processed high_school_macroeconomics.jsonl → high_school_macroeconomics_fixed.jsonl\n",
      "Processed econometrics.jsonl → econometrics_fixed.jsonl\n",
      "Processed professional_psychology.jsonl → professional_psychology_fixed.jsonl\n",
      "Processed security_studies.jsonl → security_studies_fixed.jsonl\n",
      "Processed electrical_engineering.jsonl → electrical_engineering_fixed.jsonl\n",
      "Processed nutrition.jsonl → nutrition_fixed.jsonl\n",
      "Processed miscellaneous.jsonl → miscellaneous_fixed.jsonl\n",
      "Processed management.jsonl → management_fixed.jsonl\n",
      "Processed machine_learning.jsonl → machine_learning_fixed.jsonl\n",
      "Processed virology.jsonl → virology_fixed.jsonl\n",
      "Processed us_foreign_policy.jsonl → us_foreign_policy_fixed.jsonl\n",
      "Processed logical_fallacies.jsonl → logical_fallacies_fixed.jsonl\n",
      "Processed professional_law.jsonl → professional_law_fixed.jsonl\n",
      "Processed international_law.jsonl → international_law_fixed.jsonl\n",
      "Processed human_aging.jsonl → human_aging_fixed.jsonl\n",
      "Processed jurisprudence.jsonl → jurisprudence_fixed.jsonl\n",
      "Processed moral_disputes.jsonl → moral_disputes_fixed.jsonl\n",
      "Processed global_facts.jsonl → global_facts_fixed.jsonl\n",
      "Processed high_school_european_history.jsonl → high_school_european_history_fixed.jsonl\n",
      "Processed high_school_microeconomics.jsonl → high_school_microeconomics_fixed.jsonl\n",
      "Processed elementary_mathematics.jsonl → elementary_mathematics_fixed.jsonl\n",
      "Processed abstract_algebra.jsonl → abstract_algebra_fixed.jsonl\n",
      "Processed human_sexuality.jsonl → human_sexuality_fixed.jsonl\n",
      "Processed professional_medicine.jsonl → professional_medicine_fixed.jsonl\n",
      "Processed conceptual_physics.jsonl → conceptual_physics_fixed.jsonl\n",
      "Processed prehistory.jsonl → prehistory_fixed.jsonl\n",
      "Processed college_medicine.jsonl → college_medicine_fixed.jsonl\n",
      "Processed public_relations.jsonl → public_relations_fixed.jsonl\n",
      "Processed high_school_government_and_politics.jsonl → high_school_government_and_politics_fixed.jsonl\n",
      "Processed college_computer_science.jsonl → college_computer_science_fixed.jsonl\n",
      "Processed high_school_world_history.jsonl → high_school_world_history_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"./LayerSkip/custom_datasets/mmlu\")\n",
    "\n",
    "for input_path in input_dir.glob(\"*.jsonl\"):\n",
    "    # build the corresponding output path: e.g. \"foo.jsonl\" → \"foo_fixed.jsonl\"\n",
    "    output_path = input_path.with_name(f\"{input_path.stem}_fixed.jsonl\")\n",
    "\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as fin, \\\n",
    "         output_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            record = json.loads(line)\n",
    "            # ensure `answer` is a string\n",
    "            record[\"answer\"] = str(record.get(\"answer\", \"\"))\n",
    "            fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Processed {input_path.name} → {output_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
