{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/cs7643/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'subject', 'choices', 'answer'],\n",
       "    num_rows: 14042\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "mmlu = load_dataset(\"cais/mmlu\", \"all\")\n",
    "mmlu[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add the \"prompt\" field\n",
    "def add_prompt(example):\n",
    "    # Create a formatted string for the choices\n",
    "    formatted_choices = '\\n'.join([f\"({label}) {choice}\" for label, choice in zip(choice_labels, example[\"choices\"])])\n",
    "    # Concatenate the question and the formatted choices into a new field called \"prompt\"\n",
    "    example[\"prompt\"] = example[\"question\"] + \"\\n\" + formatted_choices\n",
    "    return example\n",
    "\n",
    "# Use the .map() method to apply the function to each row in the \"test\" split\n",
    "mmlu[\"test\"] = mmlu[\"test\"].map(add_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.',\n",
       " 'subject': 'abstract_algebra',\n",
       " 'choices': ['0', '4', '2', '6'],\n",
       " 'answer': 1,\n",
       " 'prompt': 'Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\\n(0) 0\\n(1) 4\\n(2) 2\\n(3) 6'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 15/15 [00:00<00:00, 144.70ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14150821"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu[\"test\"].to_json(\"./LayerSkip/custom_datasets/mmlu_test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique subjects from the test split.\n",
    "# You can do this by extracting the \"subject\" column and converting it to a set.\n",
    "unique_subjects = set(mmlu[\"test\"][\"subject\"])\n",
    "\n",
    "# Create the output directory if it doesn't exist.\n",
    "output_folder = \"./LayerSkip/custom_datasets/mmlu/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop over each unique subject and filter the dataset.\n",
    "for subject in unique_subjects:\n",
    "    # Filter the dataset to only the rows for the current subject.\n",
    "    subject_ds = mmlu[\"test\"].filter(lambda example: example[\"subject\"] == subject)\n",
    "    \n",
    "\n",
    "    # Create a safe filename from the subject name.\n",
    "    safe_subject = subject.replace(\" \", \"_\")\n",
    "    output_file = os.path.join(output_folder, f\"{safe_subject}.jsonl\")\n",
    "    \n",
    "    # Save the filtered dataset to a JSONL file.\n",
    "    # Here we export records in a line-delimited JSON format.\n",
    "    subject_ds.to_json(output_file, orient=\"records\", lines=True)\n",
    "    \n",
    "    print(f\"Saved subject '{subject}' to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NQ-Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load the validation split\n",
    "dataset = load_dataset(\"google-research-datasets/nq_open\", split=\"validation\")\n",
    "\n",
    "# Open a file for writing in line-delimited JSON format\n",
    "with open(\"./LayerSkip/custom_datasets/nq_open_val.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dataset:\n",
    "        question = item[\"question\"]\n",
    "        answers = item[\"answer\"]\n",
    "        if answers:\n",
    "            answer = answers[0]\n",
    "            json_line = {\"question\": question, \"answer\": answer}\n",
    "            f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RACE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import ast\n",
    "\n",
    "dataset = load_dataset(\"EleutherAI/race\", split=\"test\")\n",
    "option_labels = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "with open(\"./LayerSkip/custom_datasets/race_test.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in dataset:\n",
    "        article = row[\"article\"]\n",
    "        data_str = row[\"problems\"]\n",
    "\n",
    "        try:\n",
    "            data = ast.literal_eval(data_str)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                data = json.loads(data_str.replace(\"'\", '\"'))\n",
    "            except Exception as e:\n",
    "                raise\n",
    "\n",
    "        for item in data:\n",
    "            options_str = \" \".join([f\"{label}. {opt}\" for label, opt in zip(option_labels, item['options'])])\n",
    "            question = f\"Article: {article} Question: {item['question']} Answer Options: {options_str}\"\n",
    "            answer = item[\"answer\"]\n",
    "            if answer:\n",
    "                json_line = {\"question\": question, \"answer\": answer}\n",
    "                f.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
