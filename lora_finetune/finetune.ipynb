{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/cs7643/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(\"gsm8k\", \"main\")\n",
    "datasets[\"train\"][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/layerskip-llama2-7B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 6804, 526, 278, 18577, 29973], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Where are the plants?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the maximum length of a question\n",
    "max_len_q = 0\n",
    "for example in datasets[\"train\"]:\n",
    "    max_len_q = max(len(example['question']), max_len_q)\n",
    "for example in datasets[\"test\"]:\n",
    "    max_len_q = max(len(example['question']), max_len_q)\n",
    "max_len_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_ans = 0\n",
    "for example in datasets[\"train\"]:\n",
    "    max_len_ans = max(len(example['answer']), max_len_ans)\n",
    "for example in datasets[\"test\"]:\n",
    "    max_len_ans = max(len(example['answer']), max_len_ans)\n",
    "max_len_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 1024 # The maximum length of a feature (question and context)\n",
    "\n",
    "\n",
    "def prepare_train_features(example):\n",
    "    text_column = \"question\"\n",
    "    label_column = \"answer\"\n",
    "    batch_size = len(example[text_column])\n",
    "    inputs = f\"{text_column} : {example[text_column]} Label : \"\n",
    "    targets = example[label_column]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "\n",
    "    sample_input_ids = model_inputs[\"input_ids\"]\n",
    "    label_input_ids = labels[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    # print(i, sample_input_ids, label_input_ids)\n",
    "    model_inputs[\"input_ids\"] = sample_input_ids + label_input_ids\n",
    "    labels[\"input_ids\"] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "    model_inputs[\"attention_mask\"] = [1] * len(model_inputs[\"input_ids\"])\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "features = prepare_train_features(datasets['train'][0])\n",
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "tokenized_train_dataset = datasets[\"train\"].map(prepare_train_features, remove_columns=datasets[\"train\"].column_names)\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 151 at dim 1 (got 121)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/data/data_collator.py:93\u001b[39m, in \u001b[36mdefault_data_collator\u001b[39m\u001b[34m(features, return_tensors)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/data/data_collator.py:159\u001b[39m, in \u001b[36mtorch_default_data_collator\u001b[39m\u001b[34m(features)\u001b[39m\n\u001b[32m    157\u001b[39m             batch[k] = torch.from_numpy(np.stack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m             batch[k] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mValueError\u001b[39m: expected sequence of length 151 at dim 1 (got 121)"
     ]
    }
   ],
   "source": [
    "for ex in train_dataloader:\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024 # The maximum length of a feature (question and context)\n",
    "import torch\n",
    "\n",
    "# def prepare_test_features(example):\n",
    "#     text_column = \"question\"\n",
    "#     batch_size = len(example[text_column])\n",
    "#     inputs = f\"{text_column} : {example[text_column]} Label : \"\n",
    "#     model_inputs = tokenizer(inputs)\n",
    "\n",
    "#     sample_input_ids = model_inputs[\"input_ids\"]\n",
    "#     # print(i, sample_input_ids, label_input_ids)\n",
    "#     model_inputs[\"input_ids\"] = sample_input_ids\n",
    "#     model_inputs[\"attention_mask\"] = [1] * len(model_inputs[\"input_ids\"])\n",
    "    \n",
    "#     model_inputs[\"input_ids\"] = torch.tensor(model_inputs[\"input_ids\"])\n",
    "#     model_inputs[\"attention_mask\"] = torch.tensor(model_inputs[\"attention_mask\"])\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    \n",
    "#     return model_inputs\n",
    "# features = prepare_test_features(datasets['test'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_test_dataset = datasets[\"test\"].map(prepare_train_features, remove_columns=datasets[\"test\"].column_names)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"question : Brian's friend Bobby has 5 fewer than 3 times as many video games as Brian does.  If Brian has 20 video games but lost 5 right before the comparison was made, how many does Bobby have? Label :  If Brian previously had 20 video games but lost 5, that means he now has 20-5=<<20-5=15>>15 video games\\nBrian has 15 video games, so if Bobby has 5 fewer than 3 times as many as Brian does we must first perform 15*3=45\\nWe then subtract 5 from the previous total for 45-5=40 games in total.\\n#### 40\"]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m q = tokenizer.batch_decode(ex[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].detach().cpu().numpy(), skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(q)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ans = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(ans)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3831\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3831\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3838\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3870\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3867\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3868\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3870\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3875\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:668\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    667\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    671\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    674\u001b[39m )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mOverflowError\u001b[39m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "for ex in test_dataloader:\n",
    "    # print(ex)\n",
    "    q = tokenizer.batch_decode(ex['input_ids'].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    print(q)\n",
    "    ans = tokenizer.batch_decode(ex['labels'].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    print(ans)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"facebook/layerskip-llama2-7B\",\n",
    "        use_safetensors=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 6,746,279,936 || trainable%: 0.1166\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType\n",
    "peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=30)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
    "lr = 3e-2\n",
    "num_epochs = 5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7473"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [05:46<00:00, 21.58it/s]\n",
      "100%|██████████| 1319/1319 [00:24<00:00, 54.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.9895, device='cuda:0') train_epoch_loss=tensor(0.6879, device='cuda:0') eval_ppl=tensor(1.7078, device='cuda:0') eval_epoch_loss=tensor(0.5352, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [05:46<00:00, 21.55it/s]\n",
      "100%|██████████| 1319/1319 [00:24<00:00, 54.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(1.6220, device='cuda:0') train_epoch_loss=tensor(0.4836, device='cuda:0') eval_ppl=tensor(1.5768, device='cuda:0') eval_epoch_loss=tensor(0.4554, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [05:46<00:00, 21.56it/s]\n",
      "100%|██████████| 1319/1319 [00:24<00:00, 54.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(1.5395, device='cuda:0') train_epoch_loss=tensor(0.4315, device='cuda:0') eval_ppl=tensor(1.5404, device='cuda:0') eval_epoch_loss=tensor(0.4320, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [05:49<00:00, 21.39it/s]\n",
      "100%|██████████| 1319/1319 [00:23<00:00, 54.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(1.4840, device='cuda:0') train_epoch_loss=tensor(0.3947, device='cuda:0') eval_ppl=tensor(1.5091, device='cuda:0') eval_epoch_loss=tensor(0.4115, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7473/7473 [05:46<00:00, 21.56it/s]\n",
      "100%|██████████| 1319/1319 [00:24<00:00, 54.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(1.3991, device='cuda:0') train_epoch_loss=tensor(0.3359, device='cuda:0') eval_ppl=tensor(1.4945, device='cuda:0') eval_epoch_loss=tensor(0.4018, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# training and evaluation\n",
    "device = \"cuda\"\n",
    "model = model.to(\"cuda\")\n",
    "model = model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        #         print(batch[\"input_ids\"].shape)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(test_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"llama_finetuned_gsm8k\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/layer_skip/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id = \"llama_finetuned_gsm8k\"\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy.  She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed.  In the afternoon, she gives her chickens another 25 cups of feed.  How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens?\n",
      "{'input_ids': tensor([[    1,  1139,   584,  6498,   322, 29871, 29941,   310,   670,  7875,\n",
      "          1797, 29871, 29955,   282,  4981,   294,   363,   301,  3322, 29889,\n",
      "          7806,   282, 24990,   338,  5700,   964, 29871, 29947,   269, 29399,\n",
      "         29889,   960,  6498,   322,   670,  7875,   864,   304,  6232,   278,\n",
      "           282,  4981,   294, 18018, 29892,   920,  1784,   269, 29399,   508,\n",
      "          1269,   310,   963,   505, 29973, 15796,   584, 29871]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[    1,  1139,   584,  6498,   322, 29871, 29941,   310,   670,  7875,\n",
      "          1797, 29871, 29955,   282,  4981,   294,   363,   301,  3322, 29889,\n",
      "          7806,   282, 24990,   338,  5700,   964, 29871, 29947,   269, 29399,\n",
      "         29889,   960,  6498,   322,   670,  7875,   864,   304,  6232,   278,\n",
      "           282,  4981,   294, 18018, 29892,   920,  1784,   269, 29399,   508,\n",
      "          1269,   310,   963,   505, 29973, 15796,   584, 29871, 29871, 29955,\n",
      "           282,  4981,   294,   505, 29871, 29955,   921, 29871]],\n",
      "       device='cuda:0')\n",
      "['question : Henry and 3 of his friends order 7 pizzas for lunch. Each pizza is cut into 8 slices. If Henry and his friends want to share the pizzas equally, how many slices can each of them have? Label :  7 pizzas have 7 x ']\n",
      "If each chicken eats 3 cups of feed per day, then for 20 chickens they would need 3*20=<<3*20=60>>60 cups of feed per day.\n",
      "If she feeds the flock 15 cups of feed in the morning, and 25 cups in the afternoon, then the final meal would require 60-15-25=<<60-15-25=20>>20 cups of chicken feed.\n",
      "#### 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/rmehta98/.conda/envs/cs7643/lib/python3.13/site-packages/peft/peft_model.py:1926: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "i = 4\n",
    "inputs = tokenizer( f\"question : {example['question']} Label : \", return_tensors=\"pt\")\n",
    "print(datasets[\"test\"][i][\"question\"])\n",
    "print(inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n",
    "    print(datasets['test'][i]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1139,   584,  7991,  4687,   714,   278,  4723,   411,   395,\n",
      "         29947, 29900, 29889,  1551, 27822,  7432, 29892,   540, 10398,  4203,\n",
      "           278,  6909, 29889,  1551,   323,  1041,  3250, 29892,   540, 10398,\n",
      "           697, 29899, 28491,   386,   310,   278,  5253,  2175,   515, 27822,\n",
      "         29889,  1551, 15050,  4515,  3250, 29892,   540, 10398, 29871, 29941,\n",
      "         29914, 29947,   386, 29879,   310,   278,  5253,  2175,   515,   323,\n",
      "          1041,  3250, 29889,  1128,  1568,   947,   540,   505,  2175,  1286,\n",
      "         29973, 15796,   584, 29871,  1551, 27822, 29892,   540, 10398,  4203,\n",
      "           310,   395, 29947, 29900, 10124,  1075,   411,   395, 29947, 29900,\n",
      "         17722, 29896, 29914, 29906, 11877, 29938, 29947, 29900,   353,   395,\n",
      "          9314, 29947, 29900, 17722, 29896, 29914, 29906, 11877, 29947, 29900,\n",
      "         29922, 29946, 29900,  6778, 29946, 29900,    13,  2951,   323,  1041,\n",
      "          3250, 29892,   540, 10398, 29871, 29896, 29914, 29945,   386,   310,\n",
      "           395, 29946, 29900, 10124,   395, 29946, 29900, 17722, 29896, 29914,\n",
      "         29945, 11877, 29946, 29900,   353,   395,  9314, 29946, 29900, 17722,\n",
      "         29896, 29914, 29945, 11877, 29946, 29900, 29922, 29941, 29906,  6778,\n",
      "         29941, 29906,    13,  2951, 15050,  4515,  3250, 29892,   540, 10398,\n",
      "         29871, 29941, 29914, 29947,   386, 29879,   310,   395, 29941, 29906,\n",
      "         10124,   395, 29941, 29906, 17722, 29941, 29914, 29947, 11877, 29938,\n",
      "         29941, 29906,   353,   395, 29906, 29900,    13,  4136, 29871, 29906,\n",
      "         29900,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  1551, 27822, 29892,   540, 10398,  4203,\n",
      "           310,   395, 29947, 29900, 10124,  1075,   411,   395, 29947, 29900,\n",
      "         17722, 29896, 29914, 29906, 11877, 29938, 29947, 29900,   353,   395,\n",
      "          9314, 29947, 29900, 17722, 29896, 29914, 29906, 11877, 29947, 29900,\n",
      "         29922, 29946, 29900,  6778, 29946, 29900,    13,  2951,   323,  1041,\n",
      "          3250, 29892,   540, 10398, 29871, 29896, 29914, 29945,   386,   310,\n",
      "           395, 29946, 29900, 10124,   395, 29946, 29900, 17722, 29896, 29914,\n",
      "         29945, 11877, 29946, 29900,   353,   395,  9314, 29946, 29900, 17722,\n",
      "         29896, 29914, 29945, 11877, 29946, 29900, 29922, 29941, 29906,  6778,\n",
      "         29941, 29906,    13,  2951, 15050,  4515,  3250, 29892,   540, 10398,\n",
      "         29871, 29941, 29914, 29947,   386, 29879,   310,   395, 29941, 29906,\n",
      "         10124,   395, 29941, 29906, 17722, 29941, 29914, 29947, 11877, 29938,\n",
      "         29941, 29906,   353,   395, 29906, 29900,    13,  4136, 29871, 29906,\n",
      "         29900,     2]])}\n",
      "['question : Bob started out the week with $80. On Monday alone, he spent half the money. On Tuesday, he spent one-fifth of the amount left from Monday. On Wednesday, he spent 3/8ths of the amount left from Tuesday. How much does he have left now? Label :  On Monday, he spent half of $80 leaving him with $80-(1/2)*$80 = $<<80-(1/2)*80=40>>40\\nOn Tuesday, he spent 1/5th of $40 leaving $40-(1/5)*40 = $<<40-(1/5)*40=32>>32\\nOn Wednesday, he spent 3/8ths of $32 leaving $32-(3/8)*$32 = $20\\n#### 20']\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m q = tokenizer.batch_decode(ex[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].detach().cpu().numpy(), skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(q)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ans = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(ans)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3831\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_decode\u001b[39m\u001b[34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_decode\u001b[39m(\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3808\u001b[39m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[33m\"\u001b[39m\u001b[33mnp.ndarray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch.Tensor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtf.Tensor\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   3811\u001b[39m     **kwargs,\n\u001b[32m   3812\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m   3813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3814\u001b[39m \u001b[33;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[32m   3815\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3828\u001b[39m \u001b[33;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[32m   3829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3830\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m-> \u001b[39m\u001b[32m3831\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3834\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3835\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3836\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[32m   3838\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3870\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3867\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3868\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3870\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3874\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3875\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/cs7643/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:668\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    667\u001b[39m     token_ids = [token_ids]\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m clean_up_tokenization_spaces = (\n\u001b[32m    671\u001b[39m     clean_up_tokenization_spaces\n\u001b[32m    672\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clean_up_tokenization_spaces\n\u001b[32m    674\u001b[39m )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[31mOverflowError\u001b[39m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "for ex in train_dataloader:\n",
    "    print(ex)\n",
    "    q = tokenizer.batch_decode(ex['input_ids'].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    print(q)\n",
    "    ans = tokenizer.batch_decode(ex['labels'].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    print(ans)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layer_skip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
